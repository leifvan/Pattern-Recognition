\documentclass{article}
\usepackage{spconf,amsmath,graphicx,amssymb}

% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}

\DeclareMathOperator*{\argmin}{argmin} % no space, limits underneath in displays
\DeclareMathOperator*{\argmax}{argmax}

\newcommand*{\skippingparagraph}{\par\vspace{\baselineskip}\noindent}

% Title.
% ------
\title{Report: Pattern Recognition}
%
% Single address.
% ---------------
\name{Leif Van Holland}
\address{University of Bonn}
%
% For example:
% ------------
%\address{School\\
%    Department\\
%    Address}
%
% Two addresses (uncomment and modify for two-address case).
% ----------------------------------------------------------
%\twoauthors
%  {A. Author-one, B. Author-two\sthanks{Thanks to XYZ agency for funding.}}
%    {School A-B\\
%    Department A-B\\
%    Address A-B}
%  {C. Author-three, D. Author-four\sthanks{The fourth author performed the work
%    while at ...}}
%    {School C-D\\
%    Department C-D\\
%    Address C-D}
%
\begin{document}
%\ninept
%
\maketitle

\section{Introduction}
\label{sec:intro}

In the last decades, unprecedented amounts of data are collected in various places throughout the analog and digital world. The amount of information renders manual interpretation unfeasible if not impossible. Therefore the topic of \emph{pattern recognition} plays an increasingly important role in modern technology. \cite{bigdata}\par
In the university lecture "Pattern Recognition," basic concepts of the field of research are presented. During the lecture, we worked on three projects that touched on some recurring problems.\par
Following we will represent our results chronologically, starting with a "warm-up" project about elementary model fitting. Next, the second project involved more approaches for regression as well as a simple algorithm for classification. Finally, in \textsc{Project 3}, we shed light on different clustering techniques and ultimately revisited linear classifiers equipped with methods to handle non-linearly separable data.\par

\section{Regression} \label{sec:regression}
The term \emph{regression} refers to a statistical method of estimating relationships between variables. Given a set of data $D = \left\{ (x_i,y_i) \right\} _{i=1}^N$, where $x_i \in \mathbb{R}^n$ and $y_i \in \mathbb{R}$, the goal is to find a set of parameters $\theta \in \mathbb{R}^k$ of a given model $y:\mathbb{R}^N \times \mathbb{R}^k \to \mathbb{R}$, such that $y$ \emph{predicts} the values $y_i$ based on $x_i$ as an input, with minimal error. In other words, we look for parameter values $\hat{\theta}$ such that
\begin{equation} \label{eq:argmin}
\hat{\theta} = \argmin_\theta E(\theta).
\end{equation}
$E$ denotes the \emph{objective function} that depends on the problem at hand. Most times however, $E$ measures a distance between the target output $y_i$ and the model prediction $y(x_i,\hat{\theta})$.
\subsection{Maximum Likelihood Estimation (MLE)} \label{sec:mle}
If we choose a probability distribution $\mathcal{N}$ based on parameters $\theta$ as our model, i.e. we suspect our data to be realizations of random variables $X_i\sim \mathcal{N}[\theta]$, we can calculate the probability of any possible realization $x_i$ of $X_i$ depending on $\theta$. We define the \emph{likelihood} $L$ that parameters $\theta$ are responsible for generating the set of data $D$ as a a function
\[ L(\theta,D) := P(\theta|D) = \frac{P(D|\theta)P(\theta)}{P(D)}. \]
If we assume that the random variables $X_i$ are i.i.d., we can further simplify that definition and rewrite the joint density $P(D)$ as a product and thus get $L(\theta,D) = \prod_{x_i\in D} P(\theta|x_i)$.
For numerical stability, one often considers the \emph{log-likelihood} function
\[ \mathcal{L}(\theta,D) = \text{ln }L(\theta,D) = \sum_{x_i\in D} \text{ln }P(\theta|x_i).\]
To find the parameters $\theta$ that are most likely to have generated $D$, we look for the \emph{maximum likelihood estimate}
\[\hat{\theta} = \argmax_\theta L(\theta, D) = \argmin_\theta - L(\theta,D) \]
\subsection{Normal and Weibull Distribution} \label{sec:normal-weibull}
In the first project we looked at two distributions applied to 1-dimensional data.
The \emph{1D normal distribution} $N[\mu,\sigma^2]$ is depending on two parameters $\mu,\sigma^2\in\mathbb{R}$ with the density function
\[f(x)=\frac{1}{\sqrt{2\pi \sigma^2}} e^{\frac{1}{2}(\frac{x-\mu}{\sigma})^2}.\]
Using the method of maximum likelihood estimation, we can specify the optimal choices $\hat{\mu}$ and $\hat{\sigma}^2$ for both parameters directly, which coincide with the \emph{sample mean} and \emph{population variance} of the given set of data respectively:
\[\hat{\mu} = \frac{1}{n}\sum_{i=1}^n x_i \:\text{ and }\: \hat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^n (x_i - \hat{\mu})^2\]
We applied this estimation to the body sizes of a set of data containing weights and heights of several attendees of an earlier installment of the "Pattern Recognition" lecture. The result can be seen in Fig. \ref{fig:body-normal-weibull}(a).

\begin{figure}[htb]

\begin{minipage}[b]{.48\linewidth}
  \centering
  \centerline{\includegraphics[width=4.0cm]{img/normal_mle}}
%  \vspace{1.5cm}
  \centerline{(a)}
\end{minipage}
\hfill
\begin{minipage}[b]{0.48\linewidth}
  \centering
  \centerline{\includegraphics[width=4.0cm]{img/weibull_mle}}
%  \vspace{1.5cm}
  \centerline{(b)}
\end{minipage}
%
\caption{(a) Resulting PDF of a normal distribution maximizing the likelihood w.r.t. the body sizes on the weight-height data; (b) MLE-fitted PDF of a Weibull distribution on Google Trends data.}
\label{fig:body-normal-weibull}
%
\end{figure}

The \emph{Weibull distribution} on the other hand uses parameters $\kappa, \alpha\in \mathbb{R}$ and is defined by the density
\[f(x) = \frac{\kappa}{\alpha}\left(\frac{x}{\alpha}\right)^{\kappa-1}e^{-(\frac{x}{\alpha})^\kappa}.\]
Estimating the parameters here is more involved, as there is no closed-form solution for the estimates $\hat{\kappa}$ and $\hat{\alpha}$. A maximum of the log-likelihood function
\[L(\alpha,\kappa) = N(\log \kappa - \kappa \log \alpha) + (\kappa-1)\sum_i \log d_i - \sum_i \left(\frac{d_i}{\alpha}\right)^{\kappa}\]
has therefore to be found numerically and we used \emph{Newton's method} initialized with $\kappa=1$ and $\alpha=1$. To optimize the calculation time we rewrote the log-likelihood using a histogram $h(x_j) = h_j$ for all occurring values $x_j$ in the data, which reduces the number of elements that the sums in $L$ are iterating over. $L(\alpha,\kappa)$ then equals the term
\[N(\log \kappa - \kappa \log \alpha) + (\kappa-1)\sum_j x_j \log h_j - \sum_j \left(\frac{x_j\cdot h_j}{\alpha}\right)^{\kappa}.\]
We used the described method on Google Trends data about global interest in the search term \emph{"myspace,"} measured every week between January 1, 2003 and March 16, 2012. Fig. \ref{fig:body-normal-weibull}(b) shows that assuming a Weibull distribution reasonably describes the overall trend, although one can clearly observe a deviation from the actual graph between week 100 and 200.


\subsection{Least Squares} \label{sec:least-squares}
A different but also common technique is to formulate the problem as a distance $\lVert Xw-y\lVert_2$ between a design matrix $X\in\mathbb{R}^{N\times d}$ times the solution vector $w\in\mathbb{R}^d$ and a target vector $y\in\mathbb{N}$ where
\[X = \begin{pmatrix}
x_1^T \\ \vdots \\ x_N^T
\end{pmatrix}\quad y = \begin{pmatrix}
y_1 \\ \vdots \\ y_N
\end{pmatrix}.\]
This distance is to be minimized, so as an objective function we choose
\begin{equation} \label{eq:least-squares}
E(w) = \lVert Xw-y \lVert_2^2
\end{equation}
whereby the norm is often squared for simplicity.\\
If $X$ has full rank, the problem becomes convex and therefore has a unique solution. It can be shown that this solution is determined by the product
\[w = X^+y \:\text{ where }\: X^+:=(X^TX)^{-1}X^T\]
denotes the \emph{pseudoinverse} of $X$. \cite{numerical-la}
\subsection{Excursion: Fractal Dimensions}
\begin{figure}[htb]

\begin{minipage}[b]{.48\linewidth}
  \centering
  \centerline{\includegraphics[width=4.0cm]{img/tree}}
%  \vspace{1.5cm}
  \centerline{(a) "tree" input image}
  \centerline{$D = 1.827$}\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{0.48\linewidth}
  \centering
  \centerline{\includegraphics[width=4.0cm]{img/lightning}}
%  \vspace{1.5cm}
  \centerline{(b) "lightning" input image}
  \centerline{$D = 1.595$}\medskip
\end{minipage}
%
\caption{Images of which we estimated the fractal dimension using the box-counting method.}
\label{fig:fractals}
%
\end{figure}
In \textsc{Task 1.5} we took a detour and estimated the \emph{fractal dimension} of objects in 2D images. Fractals are objects that exhibit a certain \emph{self-similarity} regardless of scale. Loosely speaking, the fractal dimension is a measure of change in complexity with respect to change in scale. One idea to define this mathematically is to count, for a given scale $s$, how often we need the pattern/fractal of scale $1$ to reproduce it. Say we counted that the object contains $n$ versions of itself at scale $s$, the fractal dimension $D$ amounts to
\[D = -\frac{\log n}{\log s}.\]
The method of \emph{box counting} is putting that abstract definition in concrete terms. For the discretized 2D images we were given (Fig. \ref{fig:fractals}) we first generated a binarized version of these images. For a set of scaling factors
\[ S = \left\{ \left. \frac{1}{2^i} \right\vert i\in \{1,...,9\}\right\} \]
the images were split into $2^{2i}$ equally sized boxes at every scale factor $s_i$. Next we determine $n_i$ by counting how many boxes at scale $s_i$ contain at least one foreground pixel. The  box-counting dimension $D$ of the image is then equal to the slope of the line
\[D\cdot \log \frac{1}{s_i} + b = D\cdot i + b = \log n_i.\]
We can estimate the parameters $D$ and $b$ via least squares using the design matrix $X =
\begin{pmatrix}
1 & ... & 9 \\
1 & ... & 1
\end{pmatrix}^T$
and target vector $y = (n_1, ..., n_9)^T$. The estimated dimensions for the images in Fig. \ref{fig:fractals} were: (a) $D=1.827$ and (b) $1.595$. This can be interpreted as (a) having more similarity with a plane whereas (b) being closer to being a line-like shape. \cite{pr-lecture, fractal-geometry}

\subsection{Least Squares Polynomials} \label{sec:poly-least-squares}
Fitting a $d$-dimensional polynomial $p(x) = \sum_{j=1}^d w_j x$ to a two-dimensional set of data can also be realized solving a linear system of equations. Recall the definition $D = \left\{ (x_i,y_i) \right\} _{i=1}^N$, albeit this time $(x_i,y_i)\in\mathbb{R}^2$. Evidently, the objective function to be minimized is
\begin{equation} \label{eq:poly-objfunc}
E(w) = \sum_{i=1}^N \lVert p(x_i)-y_i \lVert_2^2.
\end{equation}
In fact, we can rewrite this as a matrix product employing the so-called \emph{Vandermonde matrix} $X\in\mathbb{R}^{N\times d}$ with $x_{ij} = x_i^{j-1}$
%\[
%X = \begin{pmatrix}
%1 & x_1 & x_1^2 & ... & x_1^d \\
%\vdots & \vdots & \vdots & ... & \vdots \\
%1 & x_N & x_N^2 & ... & x_N^d
%\end{pmatrix}\in\mathbb{R}^{N\times d}
%\]
as the design matrix and the coefficients $w = (w_1,...w_d)^T$ of the polynomial as the parameters to be determined. The target vector is chosen as in Section \ref{sec:least-squares}. Equation (\ref{eq:poly-objfunc}) then is equivalent to the least squares objective function (\ref{eq:least-squares}).\\
In \textsc{Task 2.1} we applied this idea to the aforementioned weight-height data and fitted polynomials for $d\in\{1,5,10\}$. The resulting fits can then be used to predict the missing weight values in the data. See Fig. \ref{fig:polyfit} for details.

\begin{figure}[htb]

\begin{minipage}[b]{0.48\linewidth}
  \centering
  \centerline{\includegraphics[width=4cm]{img/poly1plot}}
%  \vspace{1.5cm}
  \centerline{(a) $d=1$}
\end{minipage}
\hfill
\begin{minipage}[b]{0.48\linewidth}
  \centering
  \centerline{\includegraphics[width=4cm]{img/poly5plot}}
%  \vspace{1.5cm}
  \centerline{(b) $d=5$}
\end{minipage}
%
\begin{minipage}[b]{\linewidth}
  \centering
  \centerline{\includegraphics[width=4cm]{img/poly10plot}}
%  \vspace{1.5cm}
  \centerline{(c) $d=10$}
\end{minipage}
\caption{Polynomials of degrees 1 (a), 5 (b) and 10 (c) fitted to the data containing height and weight of subjects. Undisclosed weight data of three subjects are estimated using the model output, marked as red.}
\label{fig:polyfit}
%
\end{figure}

The approach seen above was revisited in \textsc{Task 3.5} to gain awareness of numerical instabilities that are caused by poorly conditioned matrices. Several methods can improve the stability, including (i) the selection of a robust algorithm to solve systems of equations and (ii) normalizing the data to zero mean and unit standard deviation. We used both (i) and (ii) to improve the results in \textsc{Task 2.1}.

\subsection{Probabilistic Model Fitting}
In the beginning of Section \ref{sec:regression}, we defined the model as a function so that the equality $y_i = f(x_i) + \epsilon_i$
holds for all data points $(x_i,y_i)\in D$ ($\epsilon_i$ denotes a noise term). Taking the weight-height data as an example, this assumption might be unreasonable as there are multiple subjects of a body height of 186 cm that have different body weights. In the lecture we looked at a reformulation of the model as a conditional expectation
\begin{equation} \label{eq:cond-expect}
f(x) = \mathbb{E} [y|x] = \int y\ p(y|x)\ dy.
\end{equation}
In order to compute the target model $p(y|x) = \frac{p(x,y)}{p(x)}$ we first have to determine the joint probability $p(x,y)$ by guessing a model for the data and finding the parameters for an optimal fit. Next, we can either guess a model for the distribution $p(x)$ or by integrating $p(x,y)$ over all possible values of $y$: $p(x) = \int p(x,y)\ dy$.
Consequently, we can now calculate the expected value in (\ref{eq:cond-expect}).\par
The goal of \textsc{Task 2.3} was to utilize this method on the weight-height data. As a model for the joint probability, a bi-variate Gaussian was used. Following the result in Section \ref{sec:normal-weibull}, the mean vector $\mu$ and the covariance matrix $\Sigma$ of the distribution are based on 1D Gaussians $N[\mu_h,\sigma_h^2]$ for the height values and $N[\mu_w,\sigma_w^2]$ for the weight values, respectively. We get
\[\mu = \begin{pmatrix}
\mu_h \\ \mu_w
\end{pmatrix}
\: \text{ and } \: \Sigma = \begin{pmatrix}
\sigma_h^2 & \rho \:\sigma_h \sigma_w \\
\rho \:\sigma_h \sigma_w & \sigma_w^2
\end{pmatrix}\]
where $\rho := \frac{\text{Cov}(h,w)}{\sigma_h \sigma_w}$ is the \emph{correlation coefficient} of $h$ and $w$. In the lecture it was shown that in the case of a bi-variate Gaussian, (\ref{eq:cond-expect}) simplifies to
\[\mathbb{E}[y|x] = \mu_w + \rho \frac{\sigma_w}{\sigma_h}(x-\mu_h).\]
This led to our prediction result seen in Fig. \ref{fig:bi-gauss}(a). \cite{pr-lecture}

\begin{figure}
\begin{minipage}[b]{0.48\linewidth}
  \centering
  \centerline{\includegraphics[width=4cm]{img/contours}}
%  \vspace{1.5cm}
  \centerline{(a)}
\end{minipage}
\hfill
\begin{minipage}[b]{0.48\linewidth}
  \centering
  \centerline{\includegraphics[width=4cm]{img/bayes}}
%  \vspace{1.5cm}
  \centerline{(b)}
\end{minipage}
\caption{(a) Bi-variate Gaussian fitted to the weight-height data. (b) 5th-degree polynomial fitted to the weight-height data using Bayesian regression (blue curve) compared to the least squares regression from Fig. \ref{fig:polyfit} (dashed green curve). Predicted values marked as red.}
\label{fig:bi-gauss}
\end{figure}

\subsection{Bayesian Regression}
To go even further, we can assume a probability distribution $p(\theta)$ over the model parameters $\theta$. This idea leads to the \emph{maximum a posteriori estimate} $\theta_{MAP} = \argmax_\theta p(\theta | D),$ based on the posterior probability $p(\theta | D)$ given a set of data $D$.\par
Again using a linear model to fit a 5th-degree polynomial to the weight-height data, as we did in Section \ref{sec:poly-least-squares}, one can show that an optimal choice $\hat{w}$ for the coefficient vector is
\begin{equation} \label{eq:bayes}
\hat{w} = (X^TX+\frac{\sigma^2}{\sigma_0^2})^{-1}X^Ty
\end{equation}
if we assume a Gaussian prior $p(w) \sim N(w|\mu_0, \sigma_0^2 I)$. \cite{pr-lecture}\par
We compared the results with the polynomial fit we computed in \textsc{Task 2.1} (see Section \ref{sec:poly-least-squares}). One can observe in Fig. \ref{fig:bi-gauss}(b) that the Bayesian regression (\ref{eq:bayes}) provides a smoother fit throughout the data. In contrast, for example around 183 cm, the curve estimated by least squares drops because no data points are providing contrary information. In our opinion, the Bayes estimate is more reasonable, considering we would expect the relation between height and weight to be monotonically increasing instead of exhibiting multiple modes around certain height values.

\subsection{Excursion: Boolean Functions}
For \textsc{Task 2.4}, we applied least squares to the concept of \emph{cellular automata}; a concept that is extensively discussed by Wolfram in \cite{cellauto}. In short, a one-dimensional cellular automaton consists of infinitely many cells $x_i\in\{-1,+1\}$ and a rule $f(x_{i-1}, x_i, x_{i+1})$ to update the state of each cell in the next epoch $t+1$, based on the value of the cell and its two neighbors in epoch $t$. This definition implies that there is a total of $2^{2^3} = 256$ different rules that can be defined and Wolfram enumerated them by their byte representation.\par
As every rule provides an output for any of the eight possible input configurations, we can represent every rule as a matrix $X\in\mathbb{R}^{8\times 3}$ and a target vector $y\in\mathbb{R}^8$. Firstly, we tried to run least squares on this setting, i.e. computing $w\in\mathbb{R}^3$ as a minimum of (\ref{eq:least-squares}) like we have seen in Section \ref{sec:least-squares}. $w$ gives us coefficients of a linear combination of columns in $X$ to reconstruct the target vector $y$. No perfect reconstruction was possible for the tested rule 110 and rule 126.\par
Using the notion of \emph{Boolean Fourier series expansion} (see e.g. \cite{boolana} for details), we can rewrite any rule as an inner product $f(x) = w^T\varphi$. The vector $\varphi$ is determined for every input combination $(x_1,x_2,x_3)$ as
\[\varphi(x_1,x_2,x_3) = (1,\: x_1,\: x_2,\: x_3,\: x_1x_2,\: x_1x_3,\: x_2x_3,\: x_1x_2x_3)\]
and we used this transformation on every row of $X$ to get a matrix $\Phi\in\mathbb{R}^{8\times 8}$. This time, we minimized $\lVert \Phi w - y \lVert_2$ for x and our results show that we can perfectly reconstruct rule 110 and rule 126 from $(\Phi\cdot w)$. That is a hint to our observation in Section \ref{sec:ext-linear}, where we used transformations into higher dimensions to solve a non-linearly separable problem.
\section{Classification} \label{sec:classification}
Up until now, we tried to predict a real-valued target $y = f(x)$ given our model $f$. The idea of classification is to group data points into $n$ classes $\Omega_1,...,\Omega_n$ and therefore the model $f$ is to predict the class membership expressed as a so-called \emph{label}, e.g. $y\in\{1,...,n\}$. Often the data is split into a training set $D_{train}$ and a validation set $D_{test}$ to measure the performance of a certain classification algorithm. It may only use the known labels for data points in $D_{train}$ for training. A good performance will be achieved if the algorithm predicts the labels in $D_{test}$ reasonably well.
\subsection{k-Nearest-Neighbors} \label{sec:knn}
A simple approach to tackle the classification problem is to let neighboring training points vote for the class of an unseen data point. More explicitly, for a new data point $x\in\mathbb{R}^d$ we determine $k$ points $x_{i_1},...,x_{i_k}$ so that $\lVert x - x_{i_j} \lVert_2 \: \leq \lVert x - x_{i_l} \lVert_2$ for all $j \leq k < l$. The resulting prediction $f(x)$ is the most frequent label in $\{y_{i_1},...,y_{i_k}\}$. As seen in the lecture, the choice of $k$ influences the smoothness of the class boundary the model is implying (a higher $k$ causing the boundary to be smoother). \cite{pr-lecture}\par
In \textsc{Task 2.4} we were given a set of data points labeled as members of one of two classes. The data was split into training and test sets as described above. Running the k-Nearest-Neighbors algorithm for $k\in\{1,3,5\}$ yielded the results shown in Fig. \ref{fig:knn}.

\begin{figure}[htb]

\begin{minipage}[b]{0.48\linewidth}
  \centering
  \centerline{\includegraphics[width=4cm]{img/NNdata}}
%  \vspace{1.5cm}
  \centerline{(a)}
\end{minipage}
%
\begin{minipage}[b]{0.48\linewidth}
  \centering
  \centerline{\includegraphics[width=4cm]{img/1NN}}
%  \vspace{1.5cm}
  \centerline{(b) $k=1$ (77\% acc.)}
\end{minipage}
%
\begin{minipage}[b]{0.48\linewidth}
  \centering
  \centerline{\includegraphics[width=4cm]{img/3NN}}
%  \vspace{1.5cm}
  \centerline{(c) $k=3$ (82\% acc.)}
\end{minipage}
\hfill
\begin{minipage}[b]{0.48\linewidth}
  \centering
  \centerline{\includegraphics[width=4cm]{img/5NN}}
%  \vspace{1.5cm}
  \centerline{(d) $k=5$ (83\% acc.)}
\end{minipage}
\caption{Results of the k-Nearest-Neighbors algorithm on a (a) set of data for (b) $k=1$, (c) $k=3$, and (d) $k=5$. Filled circles denote a correct prediction of the label. Unfilled circle are incorrectly predicted points (the color denotes the true label).}
\label{fig:knn}
%
\end{figure}


\par We measured the accuracy as the percentage of correctly predicted labels in $D_{test}$ and, as expected, the accuracy increases for bigger neighborhoods. We suspect that for $k>5$ the algorithm will not generate much higher accuracy values, as some points in the data are surrounded by points of the opposite label. Thus, further smoothing the class boundary will not improve the classification of these outliers.\par
Our na\"ive implementation of the algorithm determines the neighborhood of an unseen point by measuring the distance to all training points and then taking the $k$ smallest distances. This approach took about $0.02$ seconds for determining the distances needed (independently of $k$).


\subsection{k-d Trees} \label{sec:kdtrees}
To improve the na\"ive implementation, we resorted to an efficient data structure called k-dimensional tree, or \emph{k-d-tree} for short, that was first introduced by Bently in \cite{kdtreebently}. Its idea is to partition the space using $(k-1)$-dimensional axis-aligned hyperplanes, that are represented by nodes in the tree. One can show that this leads to an average run time of $O(\log N)$ for determining the nearest neighbor of an unseen query point. See \cite{pr-lecture} or \cite{kdtreebently} for details.\par
In fact, there are several possibilities for building the k-d tree from a given set of points. For \textsc{Task 2.5} we were to employ two different methods for selecting the splitting dimension of the hyperplane, (i) alternating between x- and y-axis, or (ii) splitting along dimension with higher variance, and two different methods to determining the split point by either taking (a) the median of the data, or (b) the (arithmetic) mean of the data.\par
The choice of these methods will have an impact on the number of search steps needed to find the nearest neighbor.\par
Replacing our previous method from Section \ref{sec:knn} with a 2-d tree, we measured the run times seen in Table \ref{tab:kdtree}. Evidently, the new data structure is an overall improvement compared to the average runtime of $0.02$ sec measured for our na\"ive approach. Selecting the mean value together with cycling through the split axes seems to be the preferred choice for the given data.
\begin{table}[htb]
\centering
\begin{tabular}{c | c | c}
              & (a) median & (b) mean \\ \hline
(i)  cycle    & 0.0142     & 0.0099 \\ \hline
(ii) max. var & 0.0136     & 0.0105
\end{tabular}
\caption{Average runtimes in seconds for the combinations of building methods of a k-d-tree.}
\label{tab:kdtree}
\end{table}

\section{Clustering} \label{sec:clustering}
The type of data introduced in chapter \ref{sec:classification} implied that the training set contains label information for every data point. To add another constraint, we drop this assumption, and even though we have no information about any class membership, we suppose there are $k$ classes, and every point is a member of one of these. The goal of \emph{clustering} techniques is to determine a "best guess" for a membership function $f$. The presumed classification can then be used to gain further information about the set of data.
\subsection{k-Means Clustering}
A popular algorithm used for these kinds of problems is the \emph{k-means clustering} algorithm ($k$ being the number of distinct clusters to find). For data points $D=\{x_i\}_{i=1}^N$, the goal is to determine $k$ centroids $\mu_1,...,\mu_k$, such that the objective function
\begin{equation} \label{eq:kmeans}
E(k) = \sum_{j=1}^k \sum_{_xi\in C_j} \lVert x_i - \mu_j \lVert_2
\end{equation}
is minimized. The membership of any given point $x_i$ is determined by the closest centroid, i.e.
\[C_j := \{x_i \: \big\vert \: \lVert x_i-\mu_j \lVert_2\: < \lVert x_i-\mu_k\lVert_2 \text{ for } k\neq j\}.\]
According to Hartigan in \cite{hartiganclustering}, the k-means algorithm consists of three components:
\begin{enumerate}
\item A rule for initializing centroid positions,
\item a movement rule determining the reassignment of points to a different cluster, and
\item an update rule for the centroid positions.
\end{enumerate}
The algorithm runs iteratively, applying rules 2. and 3. until some chosen convergence criteria are met. In the lecture, we have seen three approaches that define these rules differently; (a) Lloyd's algorithm, (b) Hartigan's algorithm and (c) MacQueen's algorithm. One can show that none of the above are guaranteed to find the global minimum of (\ref{eq:kmeans}), though they will minimize it locally. \cite{pr-lecture}\par
In task 3.1 we ran the three flavors of k-means on a given set of data for $k=3$ and compared the variance of the results as well as the runtime. Fig. \ref{fig:kmeans} shows example clusterings from one of several runs of the algorithms. We observed that (a) was the fastest implementation taking $0.003$ sec, followed by (c) with $0.005$ sec and (b) being much slower with $0.428$ sec on average. On the other hand, (b) seemed to produce much less variation in its results, whereas (a) and (c) displayed multiple local optima in relatively equal frequency.

\begin{figure}[htb]

\begin{minipage}[b]{0.48\linewidth}
  \centering
  \centerline{\includegraphics[width=4cm]{img/lloyd}}
%  \vspace{1.5cm}
  \centerline{(a) Lloyd}
\end{minipage}
\hfill
\begin{minipage}[b]{0.48\linewidth}
  \centering
  \centerline{\includegraphics[width=4cm]{img/hartigan}}
%  \vspace{1.5cm}
  \centerline{(b) Hartigan}
\end{minipage}
%
\centering
\begin{minipage}[b]{0.48\linewidth}
  \centering
  \centerline{\includegraphics[width=4cm]{img/macqueen}}
%  \vspace{1.5cm}
  \centerline{(c) MacQueen}
\end{minipage}
\caption{Exemplary clustering results of three different k-means approaches for $k=3$ clusters. The colors indicate the membership of every point, plus symbols denote the centroids.}
\label{fig:kmeans}
%
\end{figure}
\subsection{Spectral Clustering}
The k-means clustering algorithm will only yield acceptable results for data that cluster in spheres around centroids, whereas it fails for differently aligned data. Therefore, methods of transforming the input space were developed and the \emph{spectral clustering} as a generalization of k-means may be one step towards the issue. \cite{spectralkmeans}\par
To simplify the definitions, we assume that the goal is to find two clusters in an unlabeled set of data $D$. First of all, we create a fully-connected weighted graph $G=(V,E)$, where the vertices $x_i\in V$ are representing the data points and the edge weights $w(i,j) \geq 0$ are expressing the \emph{similarity} between vertices $x_i$ and $x_j$. To determine two clusters in that graph is to find a cut that is partitioning the vertices into two disjoint sets $A$ and $B$. To measure the (dis)similarity for a given partition, Shi et al. proposed the \emph{normalized cut} which sums the weights of cut edges and normalizes it to be robust against outliers. Finding the minimal (non-zero) normalized cut yields a desired solution to the clustering problem, as it minimizes the similarity between the classes while maintaining a high within-class similarity. \cite{normcuts}\par
Based on this, we define the \emph{Laplacian matrix} $L=D-W$ as the difference of a diagonal matrix $D$ with entries $d_{ii} := \sum_j w(i,j)$ and $W$, which is containing the edge weights $w(i,j)$. One can further show that finding the minimal normalized cut is approximated by the eigenvalue problem
\begin{equation} \label{eq:spectral-eigval}
Ly = \lambda y.
\end{equation}
For our problem, solutions $y = (y_1,...,y_N)$ of (\ref{eq:spectral-eigval}) can be interpreted as partitions where $x_i$ belongs to $A$ if $y_1 > 0$. Otherwise it belongs to $B$. In fact, the eigenvector of the smallest eigenvalue represents the trivial cut $V=A \:\dot\cup\: B$. Therefore we are interested in the eigenvector of the second-smallest eigenvalue, sometimes called \emph{Fiedler vector}. \cite{normcuts}\par
In \textsc{Task 3.2}, we applied this method to a constructed set of data that implies an apparent separation into two classes. Choosing the similarity $w(i,j) = \exp(-\beta \lVert x_i - x_j \lVert^2)$, we found that for $\beta = 4$, the spectral clustering method yields the apparent separation, while evidently k-means clustering (for $k=2$) cannot come close to this result (see Fig. \ref{fig:spectral}).

\begin{figure}
\begin{minipage}[b]{0.48\linewidth}
  \centering
  \centerline{\includegraphics[width=4cm]{img/project3/task_3_2_kmeans}}
%  \vspace{1.5cm}
  \centerline{(a)}
\end{minipage}
\begin{minipage}[b]{0.48\linewidth}
  \centering
  \centerline{\includegraphics[width=4cm]{img/project3/task_3_2_spectral4}}
%  \vspace{1.5cm}
  \centerline{(b)}
\end{minipage}
\caption{A non-linearly separable set of data separated by k-means for $k=2$ (a) and by spectral clustering (b). The colors indicate the membership of each data point to one of the two classes.}
\label{fig:spectral}
%
\end{figure}

\section{Dimensionality Reduction}
The lecture briefly discussed the so-called \emph{curse of dimensionality}, stating that for randomly distributed points in high-dimensional spaces, the distances are likely to be almost equal. This fact renders clustering algorithms depending on distances, like the ones we have seen in Section \ref{sec:classification}, less useful.\par
A simple idea is to reduce the dimension of the input space by determining an appropriate projection onto a lower-dimensional space. Finding such a projection is in itself a problem that can be solved with different objectives in mind.
\subsection{Principal Component Analysis (PCA)} \label{sec:pca}
Intuitively, one may consider the variance of the data to determine the most important dimensions. The \emph{principal components} of $D$ are defined as orthogonal directions of highest variance. These can be obtained by calculating an eigen-decomposition $\Sigma = V \Lambda V^{-1}$ of the data covariance matrix $\Sigma$. As it can be shown that $\Sigma$ is positive semi-definite, such a decomposition always exists. In fact, for such matrices, the eigenvectors of different eigenvalues are orthogonal and can be ordered because all eigenvalues are real and positive. Thereby an eigen-decomposition of $\Sigma$ naturally yields principal components. To reduce the dimension to $k$, we then project the data onto the eigenvectors of the $k$ highest eigenvalues. \cite{pca}\par
We were given a labeled 500-dimensional set of data in \textsc{Task 3.3} and applied this method to reduce the dimension to $k\in\{2,3\}$. The visualized projections in Fig. \ref{fig:dimreduction}(a) and \ref{fig:dimreduction}(c) indeed reveal clusters. We will now compare this to another method and see if we can improve the results.
\subsection{Linear Discriminant Analysis (LDA)}
As the data might be labeled, we can incorporate this information into the process. Suppose we are given data from $k$ classes, we calculate the mean $\mu_j$ and covariances $\Sigma_j$ inside each class as well as the overall mean $\mu$. Next, we define the within-class scatter matrix $S_W$ and the between-class scatter matrix $S_B$:
\[S_W = \sum_{j=1}^k \Sigma_j \qquad S_B = \sum_{j=1}^k (\mu_j-\mu)(\mu_j-\mu)^T.\]
To find a suitable projection $W$, in the lecture we showed one has to solve the following constrained minimization problem
\begin{equation} \label{eq:lda-prob}
\min_W W^TS_BW \text{ s.t. } W^TS_WW = I.
\end{equation}
Fisher proved in \cite{lda} that a matrix $W$, consisting of eigenvectors of the $k$ largest eigenvalues of $S_W^{-1}S_B$, is a solution of (\ref{eq:lda-prob}).\par
Applying this to the 500-dimensional data from Section \ref{sec:pca}, we obtain the projections visualized in Fig. \ref{fig:dimreduction}(b) and \ref{fig:dimreduction}(d). Again, we can observe that points from the same class are spatial close. Because the LDA-projection is using the labels to minimize the within-class scatter, the points are close even along the third axis, unlike when projected via PCA.

\begin{figure}[htb]

\begin{minipage}[b]{0.48\linewidth}
  \centering
  \centerline{\includegraphics[width=4cm]{img/pca_2d}}
%  \vspace{1.5cm}
  \centerline{(a)}
\end{minipage}
%
\begin{minipage}[b]{0.48\linewidth}
  \centering
  \centerline{\includegraphics[width=4cm]{img/lda_2d}}
%  \vspace{1.5cm}
  \centerline{(b)}
\end{minipage}
%
\begin{minipage}[b]{0.48\linewidth}
  \centering
  \centerline{\includegraphics[width=4cm]{img/pca_3d}}
%  \vspace{1.5cm}
  \centerline{(c)}
\end{minipage}
\hfill
\begin{minipage}[b]{0.48\linewidth}
  \centering
  \centerline{\includegraphics[width=4cm]{img/lda_3d}}
%  \vspace{1.5cm}
  \centerline{(d)}
\end{minipage}
\caption{Projections of a labeled 500-dimensional set of data into 2D (a, b) and 3D (c, d) using PCA (a, c) and LDA (b, d). Colors denote the class membership of the points.}
\label{fig:dimreduction}
\end{figure}

\section{Extending Linear Classifiers} \label{sec:ext-linear}
Lastly, we investigated the capabilities of traditionally linear classifiers, namely \emph{perceptrons} and \emph{support vector machines} extended with non-linear functions, and tested their performance on the XOR-problem. We now briefly discuss our findings.
\subsection{Non-Monotonous Neurons}
A perceptron with parameters $w$ and $\theta$ calculates the projection $y(x) = f(w^Tx-\theta)$ for a given activation function $f$. If $f$ is chosen to be monotonous, $y$ can only solve linearly separable problems, so for \textsc{Task 3.4} a non-monotonous activation function was selected, resulting in
\[y(x) = 2\exp\left(-\frac{1}{2}(w^Tx-\theta)^2\right)-1.\]
To find the optimal parameters for the given data, we performed gradient descend over the loss function
\[E=\frac{1}{2} \sum_{i=1}^N(y(x_i)-y_i)^2.\] This yielded a perfect classification of the XOR-problem as seen in Fig. \ref{fig:neurons}(b). For a comparison, in Fig. \ref{fig:neurons}(a) we visualized the result using the monotonous activation function $f(x) = \tanh(x)$ with which it is impossible to classify the data correctly.
\subsection{Support Vector Machines (SVM)}
A different linear classifier is the so-called \emph{support vector machine}. Its goal is to maximize the margin between two linearly separable classes and the separating hyperplane. Though this optimization seems involved, we can consider the dual constrained optimization problem
\[\argmax_\mu -\frac{1}{2}\mu^T G \mu + 1^T\mu \: \text{ s.t. } \: y^T\mu = 0 \text{ and } \mu \geq 0.\]
Its solution then yields the parameters for the linear classification $w = \sum_{i=1}^N \mu_i y_i x_i$. The calculation simplifies drastically when following the ideas in \cite{wolfe}, as seen in the lecture. \cite{pr-lecture} \par
To "non-linearize" this approach we can utilize the \emph{kernel trick}. The idea behind it is to project the data into a higher-dimensional space where it becomes linearly separable using a non-linear transformation $\varphi$. Finding (and potentially also computing) $\varphi$ is generally a very difficult problem, but Mercer first proved in \cite{mercer} that it suffices to find a positive semidefinite \emph{kernel function} $k(a,b)$ in the input space as it implies the existence of a transformation $\varphi$ such that $k(a,b) = \varphi(a)^T\varphi(b)$.\par
As seen in the lecture, the kernel trick can be easily applied to support vector machines. Using a quadratic kernel $k(a,b) = (a^Tb + c)^2$ we were in fact able to separate the XOR-problem using the modified SVM (see Fig. \ref{fig:neurons}(c)). \cite{pr-lecture}

\begin{figure}[htb]

\begin{minipage}[b]{0.48\linewidth}
  \centering
  \centerline{\includegraphics[width=4cm]{img/gauss}}
%  \vspace{1.5cm}
  \centerline{(a)}
\end{minipage}
\hfill
\begin{minipage}[b]{0.48\linewidth}
  \centering
  \centerline{\includegraphics[width=4cm]{img/tanh}}
%  \vspace{1.5cm}
  \centerline{(b)}
\end{minipage}
%
\centering
\begin{minipage}[b]{0.48\linewidth}
  \centering
  \centerline{\includegraphics[width=4cm]{img/svm}}
%  \vspace{1.5cm}
  \centerline{(c)}
\end{minipage}

\caption{Outputs on an XOR-problem of classifiers with (a) non-linear activation function, (b) linear activation function, and (c) of a modified SVM.}
\label{fig:neurons}
%
\end{figure}

\section{Conclusion}
In this report, we presented our solutions for the three different projects we worked on in the course of the lecture. In \textsc{Project 1} one we have seen how to fit simple and more complex probability distributions to data using maximum likelihood estimation. We also applied linear regression to estimate the fractal dimension of given images.\par
\textsc{Project 2} was about different methods to predict missing values, namely via least squares polynomials, conditional expectation, and Bayesian regression. We concluded that, though the computation is more involved, the Bayesian approach yielded a superior model compared to the other methods. Next, we looked at the nearest-neighbor classifier and improved the runtime using k-d-trees.\par
Finally in \textsc{Project 3}, we applied two clustering algorithms: k-means and spectral clustering. As they are prone to deliver suboptimal results for high-dimensional input spaces, we used principal component analysis and Fisher's linear discriminant analysis for dimensionality reduction. Furthermore, we took two classic linear classifiers, (i) the perceptron and (ii) the support vector machine, and implemented approaches to be capable of handling non-linearly separable data. For (i) we used a non-linear activation function and for (ii) we utilized the kernel trick to accomplish the goal. Both techniques were then able to solve the XOR-problem.\par
All in all the work showed that, though a multitude of approaches exist to analyze data in various ways today, one has to be aware of pitfalls when using computers, as they are prone to numerical errors. Also, there is often no obvious approach to choose for the problem at hand. Nevertheless pattern recognition provides a powerful toolset of which we have only scratched the surface.


% References should be produced using the bibtex program from suitable
% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------
\bibliographystyle{IEEEbib}
\bibliography{literature}

\end{document}
